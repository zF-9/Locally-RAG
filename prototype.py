# Import required libraries
from langchain_ollama import  OllamaLLM #,OllamaEmbeddings
from pypdf import PdfReader
import chromadb
import os
from chromadb.utils.embedding_functions.ollama_embedding_function import OllamaEmbeddingFunction

# Define the LLM model to be used
llm_model = "llama3.2"

#collect context from user 
documents = []
prompt = []


reader = PdfReader('sample_pdf\Introduction_to_Machine_Learning.pdf')
print("extracted pages for input documents are " + str(len(reader.pages)) + " pages")

for x in range(0, len(reader.pages)):
    # extracting text from page
    page = reader.pages[x]
    ext_text = page.extract_text()

print("ze documants isa digested successfoolay!")


# Configure ChromaDB
# Initialize the ChromaDB client with persistent storage in the current directory
x_path = path = path=os.path.join(os.getcwd(), "chroma_db")
chroma_client = chromadb.PersistentClient(x_path)

# Define a custom embedding function for ChromaDB using Ollama
class ChromaDBEmbeddingFunction:
    """
    Custom embedding function for ChromaDB using embeddings from Ollama.
    """
    def __init__(self, langchain_embeddings):
        self.langchain_embeddings = langchain_embeddings

    def __call__(self, input):
        # Ensure the input is in a list format for processing
        if isinstance(input, str):
            input = [input]
        return self.langchain_embeddings.embed_documents(input)

# Initialize the embedding function with Ollama embeddings
embedding = OllamaEmbeddingFunction( #ChromaDBEmbeddingFunction(
    #OllamaEmbeddings(
        model_name=llm_model,
        url="http://localhost:11434"  # Adjust the base URL as per your Ollama server configuration
    #)
)

# Define a collection for the RAG workflow
collection_name = "rag_collection"
collection = chroma_client.get_or_create_collection(
    name=collection_name,
    metadata={"description": "A collection for RAG with Ollama - Demo_#I.LOST.COUNT"},
    embedding_function=embedding  # Use the custom embedding function
)

# User add additional context or prompting 
# Prompt the user to enter text repeatedly until an empty input is provided
while True:
    text_input = input("load additional information (or press Enter to finish): ")

    # Check if the input is empty, indicating the user wants to stop
    if text_input == "":
        break  # Exit the loop

    # Add the non-empty input to the list
    prompt.append(text_input)

# Function to add documents to the ChromaDB collection
def add_documents_to_collection(documents, ids): 
    """
    Add documents to the ChromaDB collection.
    
    Args:
        documents (list of str): The documents to add.
        ids (list of str): Unique IDs for the documents.
    """
    collection.add(
        documents=documents,
        ids=ids
    )


# Document & Prompt pre-process
documents = [ext_text]
prompt_c = len(prompt)

doc_ids = []
for i in range (0, prompt_c + 1):
    doc_ids.append(f"DOC-{i:03d}")


# Print the collected inputs
print("\nYour collected context are:")
watch_ = documents + prompt
watch = watch_[::-1]

documents = watch

print(documents)
print(doc_ids)


# Documents only need to be added once or whenever an update is required. 
# This line of code is included for demonstration purposes:
add_documents_to_collection(documents, doc_ids)


# Function to query the ChromaDB collection
def query_chromadb(query_text, n_results=1):
    """
    Query the ChromaDB collection for relevant documents.
    
    Args:
        query_text (str): The input query.
        n_results (int): The number of top results to return.
    
    Returns:
        list of dict: The top matching documents and their metadata.
    """
    results = collection.query(
        query_texts=[query_text],
        n_results=n_results
    )
    return results["documents"], results["metadatas"]

# Function to interact with the Ollama LLM
def query_ollama(prompt):
    """
    Send a query to Ollama and retrieve the response.
    
    Args:
        prompt (str): The input prompt for Ollama.
    
    Returns:
        str: The response from Ollama.
    """
    llm = OllamaLLM(model=llm_model)
    return llm.invoke(prompt)

# RAG pipeline: Combine ChromaDB and Ollama for Retrieval-Augmented Generation
def rag_pipeline(query_text):
    """
    Perform Retrieval-Augmented Generation (RAG) by combining ChromaDB and Ollama.
    
    Args:
        query_text (str): The input query.
    
    Returns:
        str: The generated response from Ollama augmented with retrieved context.
    """
    # Step 1: Retrieve relevant documents from ChromaDB
    retrieved_docs, metadata = query_chromadb(query_text)
    context = " ".join(retrieved_docs[0]) if retrieved_docs else "No relevant documents found."

    # Step 2: Send the query along with the context to Ollama
    augmented_prompt = f"Context: {context}\n\nQuestion: {query_text}\nAnswer:"
    print("######## Augmented Prompt ########")
    print(augmented_prompt)

    response = query_ollama(augmented_prompt)
    return response

# Example usage
# Define a query to test the RAG pipeline
#query = "what is the Linux requirement to run Tauri framework on your machine"  # Change the query as needed
query = input("Enter your query: \n")
response = rag_pipeline(query)
print("######## Response from LLM ########\n", response)